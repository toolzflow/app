import { Tables } from "@/supabase/types"
import { ChatPayload, MessageImage } from "@/types"
import { encode } from "gpt-tokenizer"
import { CHAT_SETTING_LIMITS } from "@/lib/chat-setting-limits"

export const DEFAULT_SYSTEM_PROMPT = `Today is {local_date}.
User info: "{profile_context}"
{assistant}`

export function validateSystemPromptTemplate(template: string) {
  return (
    template.includes("{profile_context}") &&
    template.includes("{local_date}") &&
    template.includes("{assistant}")
  )
}

export const buildBasePrompt = (
  profileContext: string,
  assistant: Tables<"assistants"> | null,
  template = DEFAULT_SYSTEM_PROMPT
) => {
  return template
    .replace("{local_date}", new Date().toLocaleDateString())
    .replace("{profile_context}", profileContext)
    .replace(
      "{assistant}",
      assistant ? `You are ${assistant.name}.\n\n ${assistant.prompt}` : ""
    )
}

export async function buildFinalMessages(
  payload: ChatPayload,
  profile: Tables<"profiles">,
  chatImages: MessageImage[]
) {
  const {
    chatSettings,
    chatMessages,
    assistant,
    messageFileItems,
    chatFileItems
  } = payload

  const BUILT_PROMPT = buildBasePrompt(
    profile?.profile_context || "",
    assistant,
    profile?.system_prompt_template || DEFAULT_SYSTEM_PROMPT
  )

  let CHUNK_SIZE = 4096 // sane default
  if (chatSettings.contextLength) {
    CHUNK_SIZE = chatSettings.contextLength
  }

  if (chatSettings.model in CHAT_SETTING_LIMITS) {
    CHUNK_SIZE = CHAT_SETTING_LIMITS[chatSettings.model].MAX_CONTEXT_LENGTH
  }

  const PROMPT_TOKENS = encode(chatSettings.prompt).length

  let remainingTokens = CHUNK_SIZE - PROMPT_TOKENS

  let usedTokens = 0
  usedTokens += PROMPT_TOKENS

  const processedChatMessages = chatMessages.map((chatMessage, index) => {
    const nextChatMessage = chatMessages[index + 1]

    if (nextChatMessage === undefined) {
      return chatMessage
    }

    const nextChatMessageFileItems = nextChatMessage.fileItems

    if (nextChatMessageFileItems.length > 0) {
      const findFileItems = nextChatMessageFileItems
        .map(fileItemId =>
          chatFileItems.find(chatFileItem => chatFileItem.id === fileItemId)
        )
        .filter(item => item !== undefined) as Tables<"file_items">[]

      const retrievalText = buildRetrievalText(findFileItems)

      return {
        message: {
          ...chatMessage.message,
          content:
            `${chatMessage.message.content}\n\n${retrievalText}` as string
        },
        fileItems: []
      }
    }

    return chatMessage
  })

  let finalMessages = []

  for (let i = processedChatMessages.length - 1; i >= 0; i--) {
    const message = processedChatMessages[i].message
    const messageTokens = encode(message.content).length

    if (messageTokens <= remainingTokens) {
      remainingTokens -= messageTokens
      usedTokens += messageTokens
      finalMessages.unshift(message)
    } else {
      break
    }
  }

  let tempSystemMessage: Tables<"messages"> = {
    chat_id: "",
    assistant_id: null,
    content: BUILT_PROMPT,
    created_at: "",
    id: processedChatMessages.length + "",
    image_paths: [],
    model: payload.chatSettings.model,
    role: "system",
    sequence_number: processedChatMessages.length,
    updated_at: "",
    user_id: "",
    annotation: {},
    word_count: 0
  }

  finalMessages.unshift(tempSystemMessage)

  finalMessages = finalMessages.map(message => {
    let content

    if (message.image_paths.length > 0) {
      content = [
        {
          type: "text",
          text: message.content
        },
        ...message.image_paths.map(path => {
          let formedUrl = ""

          if (path.startsWith("data")) {
            formedUrl = path
          } else {
            const chatImage = chatImages.find(image => image.path === path)

            if (chatImage) {
              formedUrl = chatImage.base64
            }
          }

          if (chatSettings.model.indexOf("claude-3") !== -1) {
            // parse data:image/png;base64 media_type
            try {
              const data = formedUrl.split(",")
              const mediaType = data[0].split(";")[0].split(":")[1]
              return {
                type: "image",
                source: {
                  type: "base64",
                  media_type: mediaType,
                  data: data[1]
                }
              }
            } catch (error) {
              console.log(error)
            }
          }

          if (chatSettings.model.indexOf("gpt-4-turbo") !== -1) {
            return {
              type: "image_url",
              image_url: {
                url: formedUrl
              }
            }
          }

          return {
            type: "image_url",
            image_url: {
              url: formedUrl
            }
          }
        })
      ]
    } else {
      content = message.content
    }

    return {
      role: message.role,
      content
    }
  })

  if (messageFileItems.length > 0) {
    const retrievalText = buildRetrievalText(messageFileItems)

    finalMessages[finalMessages.length - 1] = {
      ...finalMessages[finalMessages.length - 1],
      content: `${
        finalMessages[finalMessages.length - 1].content
      }\n\n${retrievalText}`
    }
  }

  return { finalMessages, usedTokens }
}

function buildRetrievalText(fileItems: Tables<"file_items">[]) {
  const retrievalText = fileItems
    .map(item => `<BEGIN SOURCE>\n${item.content}\n</END SOURCE>`)
    .join("\n\n")

  return `You may use the following sources if needed to answer the user's question. If you don't know the answer, say "I don't know."\n\n${retrievalText}`
}

export async function buildGoogleGeminiFinalMessages(
  payload: ChatPayload,
  profile: Tables<"profiles">,
  messageImageFiles: MessageImage[]
) {
  const { chatSettings, chatMessages, assistant } = payload

  const BUILT_PROMPT = buildBasePrompt(
    profile?.profile_context || "",
    assistant,
    profile?.system_prompt_template || DEFAULT_SYSTEM_PROMPT
  )

  let finalMessages = []

  let usedTokens = 0
  const CHUNK_SIZE = CHAT_SETTING_LIMITS[chatSettings.model].MAX_CONTEXT_LENGTH
  const PROMPT_TOKENS = encode(chatSettings.prompt).length
  let REMAINING_TOKENS = CHUNK_SIZE - PROMPT_TOKENS

  usedTokens += PROMPT_TOKENS

  for (let i = chatMessages.length - 1; i >= 0; i--) {
    const message = chatMessages[i].message
    const messageTokens = encode(message.content).length

    if (messageTokens <= REMAINING_TOKENS) {
      REMAINING_TOKENS -= messageTokens
      usedTokens += messageTokens
      finalMessages.unshift(message)
    } else {
      break
    }
  }

  let tempSystemMessage: Tables<"messages"> = {
    chat_id: "",
    assistant_id: null,
    content: BUILT_PROMPT,
    created_at: "",
    id: chatMessages.length + "",
    image_paths: [],
    model: payload.chatSettings.model,
    role: "system",
    sequence_number: chatMessages.length,
    updated_at: "",
    user_id: "",
    annotation: {},
    word_count: 0
  }

  finalMessages.unshift(tempSystemMessage)

  let GOOGLE_FORMATTED_MESSAGES: any[] = []

  async function fileToGenerativePart(file: File) {
    const base64EncodedDataPromise = new Promise(resolve => {
      const reader = new FileReader()

      reader.onloadend = () => {
        if (typeof reader.result === "string") {
          resolve(reader.result.split(",")[1])
        }
      }

      reader.readAsDataURL(file)
    })

    return {
      inlineData: {
        data: await base64EncodedDataPromise,
        mimeType: file.type
      }
    }
  }

  if (
    chatSettings.model === "gemini-pro" ||
    chatSettings.model === "gemini-1.5-pro-latest" ||
    chatSettings.model === "gemini-1.5-flash-latest"
  ) {
    GOOGLE_FORMATTED_MESSAGES = [
      {
        role: "user",
        parts: [
          {
            text: finalMessages[0].content
          }
        ]
      },
      {
        role: "model",
        parts: [
          {
            text: "I will follow your instructions."
          }
        ]
      }
    ]

    for (let i = 1; i < finalMessages.length; i++) {
      GOOGLE_FORMATTED_MESSAGES.push({
        role: finalMessages[i].role === "user" ? "user" : "model",
        parts: [
          {
            text: finalMessages[i].content as string
          }
        ]
      })
    }

    const files = messageImageFiles.map(file => file.file)

    const imageParts = await Promise.all(
      files.flatMap(file => (file ? [fileToGenerativePart(file)] : []))
    )

    GOOGLE_FORMATTED_MESSAGES[GOOGLE_FORMATTED_MESSAGES.length - 1].parts.push(
      ...imageParts
    )

    return {
      finalMessages: GOOGLE_FORMATTED_MESSAGES,
      usedTokens
    }
  } else if (chatSettings.model === "gemini-pro-vision") {
    // Gemini Pro Vision doesn't currently support messages
    let prompt = ""

    for (let i = 0; i < finalMessages.length; i++) {
      prompt += `${finalMessages[i].role}:\n${finalMessages[i].content}\n\n`
    }

    const files = messageImageFiles.map(file => file.file)
    const imageParts = await Promise.all(
      files.map(file =>
        file ? fileToGenerativePart(file) : Promise.resolve(null)
      )
    )

    // FIX: Hacky until chat messages are supported
    return {
      finalMessages: [
        {
          prompt,
          imageParts
        }
      ],
      usedTokens: encode(prompt).length
    }
  }

  return { finalMessages, usedTokens }
}

// Anthropic API requires first assistant message to be user
export async function buildClaudeFinalMessages(
  payload: ChatPayload,
  profile: Tables<"profiles">,
  chatImages: MessageImage[]
) {
  const { finalMessages, usedTokens } = await buildFinalMessages(
    payload,
    profile,
    chatImages
  )

  // Remove first assistant message
  if (
    finalMessages.length > 1 &&
    finalMessages[1].role !== "user" &&
    finalMessages[0].role === "system"
  ) {
    return {
      finalMessages: finalMessages.toSpliced(1, 1),
      usedTokens: usedTokens - encode(finalMessages[1].content as string).length
    }
  }

  return { finalMessages, usedTokens }
}
